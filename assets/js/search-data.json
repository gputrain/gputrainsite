{
  
    
        "post0": {
            "title": "Deploying my fiirst model to HuggingFace spaces",
            "content": "Know before getting started . This notebook code is the same used to create my app.py file you can find in UrbanSounds8k spaces repo. . Creating a space is simple and intuitive at Hugging Face. You will need a &quot;Create a new space&quot; and &quot;Create a new model repository&quot; at huggingfaces. You will find the interface to create a new model repository (repo) in your profile settings. . . If you upload your model artifacts into your spaces repository, you will run into 404 or 403 series errors. Once you create a model repo, the installation steps of Hugging Face will have an install git lfs in the set of instructions specific to the location you clone this empty repo. . If you upload your model artifacts into your spaces repository, you will run into 404 or 403 series errors. Once you create a model repo, the installation steps of Hugging Face will have an install git lfs in the set of instructions specific to the location you clone this empty repo. . Add in that repo before copying your model the *.pkl file from the earlier step; ensure you track pkl files as a type of file managed by Git LFS . git lfs track &quot;*.pkl&quot; . Tip: Note what is my spaces repo and what is in my model repo . If you miss this step you will run into 403 errors as you execute this line: . model_file = hf_hub_download(&quot;gputrain/UrbanSound8K-model&quot;, &quot;model.pkl&quot;) . Back to spaces repo, for which I have specific requirements.txt to be able to load librosa modules to get my inference function to work. In my example, I also needed to get my labeller function to get my model to work. This requirements.txt is my spaces repo. . import gradio from fastai.vision.all import * from fastai.data.all import * from pathlib import Path import pandas as pd from matplotlib.pyplot import specgram import librosa import librosa.display from huggingface_hub import hf_hub_download from fastai.learner import load_learner . . Matplotlib is building the font cache; this may take a moment. . ref_file = hf_hub_download(&quot;gputrain/UrbanSound8K-model&quot;, &quot;UrbanSound8K.csv&quot;) model_file = hf_hub_download(&quot;gputrain/UrbanSound8K-model&quot;, &quot;model.pkl&quot;) . . Labeller function . My labeller function and loading of the model code are below. Model loaded and vocabulary on classes retrieved from the model object. . df = pd.read_csv(ref_file) df[&#39;fname&#39;] = df[[&#39;slice_file_name&#39;,&#39;fold&#39;]].apply (lambda x: str(x[&#39;slice_file_name&#39;][:-4])+&#39;.png&#39;.strip(),axis=1 ) my_dict = dict(zip(df.fname,df[&#39;class&#39;])) def label_func(f_name): f_name = str(f_name).split(&#39;/&#39;)[-1:][0] return my_dict[f_name] model = load_learner (model_file) labels = model.dls.vocab . . Helpful external facing markdown text . This bit of code allows some text markdown to appear in your demo. It takes standard markdown for tables and text formats, which is very useful to provide some descriptive elements to your demo, from my spaces repo. . with open(&quot;article.md&quot;) as f: article = f.read() . . Inference . Interface options . The interface option allows for some text and examples objects which reside in your spaces repo. . interface_options = { &quot;title&quot;: &quot;Urban Sound 8K Classification&quot;, &quot;description&quot;: &quot;Fast AI example of using a pre-trained Resnet34 vision model for an audio classification task on the [Urban Sounds](https://urbansounddataset.weebly.com/urbansound8k.html) dataset. &quot;, &quot;article&quot;: article, &quot;interpretation&quot;: &quot;default&quot;, &quot;layout&quot;: &quot;horizontal&quot;, # Audio from validation file &quot;examples&quot;: [&quot;dog_bark.wav&quot;, &quot;children_playing.wav&quot;, &quot;air_conditioner.wav&quot;, &quot;street_music.wav&quot;, &quot;engine_idling.wav&quot;, &quot;jackhammer.wav&quot;, &quot;drilling.wav&quot;, &quot;siren.wav&quot;,&quot;car_horn.wav&quot;,&quot;gun_shot.wav&quot;], &quot;allow_flagging&quot;: &quot;never&quot; } . . Pipeline helper function . With my Urban Sound, 8K example, I have a custom transformation that takes the audio wav file and puts this through librosa to create a Melspectrogram. I am using a fixed temporary image as temp.png for my inference object. . def convert_sounds_melspectogram (audio_file): samples, sample_rate = librosa.load(audio_file) #create onces with librosa fig = plt.figure(figsize=[0.72,0.72]) ax = fig.add_subplot(111) ax.axes.get_xaxis().set_visible(False) ax.axes.get_yaxis().set_visible(False) ax.set_frame_on(False) melS = librosa.feature.melspectrogram(y=samples, sr=sample_rate) librosa.display.specshow(librosa.power_to_db(melS, ref=np.max)) filename = &#39;temp.png&#39; plt.savefig(filename, dpi=400, bbox_inches=&#39;tight&#39;,pad_inches=0) plt.close(&#39;all&#39;) return None . . Pipeline predict function . The predict function uses this temp.png file for its predictions, and the labels_probs object has a list of class probabilities. . def predict(): img = PILImage.create(&#39;temp.png&#39;) pred,pred_idx,probs = model.predict(img) return {labels[i]: float(probs[i]) for i in range(len(labels))} return labels_probs . . The Pipeleine function . My inference pipeline is done in this end2endpipeline object that converts the sound and returns the output of my predict function. . def end2endpipeline(filename): convert_sounds_melspectogram(filename) return predict() . . Launch options . I call the gradio Interface object referencing my inference pipeline displaying input types and output classes. . demo = gradio.Interface( fn=end2endpipeline, inputs=gradio.inputs.Audio(source=&quot;upload&quot;, type=&quot;filepath&quot;), outputs=gradio.outputs.Label(num_top_classes=10), **interface_options, ) . /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect warnings.warn(value) /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: The &#39;type&#39; parameter has been deprecated. Use the Number component instead. warnings.warn(value) /home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: `layout` parameter is deprecated, and it has no effect warnings.warn(value) . . Gradio local or public endpoint . The launch options launches the gradio UI. When using AWS I wasn&#39;t able to get this to work with the local URL, however if you use the share=True you get a gradio URL that works. . launch_options = { &quot;enable_queue&quot;: True, &quot;share&quot;: False, #&quot;cache_examples&quot;: True, } demo.launch(**launch_options) . . Running on local URL: http://127.0.0.1:7861/ To create a public link, set `share=True` in `launch()`. . (&lt;gradio.routes.App at 0x7f671108ce50&gt;, &#39;http://127.0.0.1:7861/&#39;, None) . Hugging Face . The final end point once you assemble all this work through the logs (to resolve errors) is here . .",
            "url": "https://www.gputrain.com/sound/hugging%20face/fastai/2022/05/26/Deploying-my-first-HuggingFace-space.html",
            "relUrl": "/sound/hugging%20face/fastai/2022/05/26/Deploying-my-first-HuggingFace-space.html",
            "date": " • May 26, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Urban Sounds 8k Classification with Hugging Face Demo",
            "content": "Hugging Face Demo . Here is a demo of what this model at Hugging Face : Urban Sound 8K Classification . Background . Urban Sounds is a dataset of 8732 labeled sounds of less than 4 seconds each from 10 classes. Dataset for UrbanSounds8K contains these 10 classes: . air_conditioner | car_horn | children_playing | dog_bark | drilling | engine_idling | gun_shot | jackhammer | siren | street_music | Research with this dataset as of 2019 and optimized ML approaches as of late 2019 had classification accuracy at 74% with a k-nearest neighbours (KNN) algorithm. A deep learning neural network trained from scratch obtained accuracy at 76% accuracy. . . (accuracy metrics for research article) . The state-of-the-art methods for audio classification approach this problem as an image classification task. For such image classification problems from audio samples, three common transformation approaches are: . Linear Spectrograms | | Log Spectrograms | | Mel Spectrograms | | . You can learn more about these three transformations in Scott Duda&#39;s article and Ketan Doshi&#39;s writing, reasoning why Mel Spectrograms perform better in general for visual transformations of audio files. . The transformation on these audio files is another notebook that I will add a write-up here. You can find all of the associated code here. . Code . Import the necessary modules . Using an AWS conda_pytorch_p38 environment with a ml.g4dn.2xlarge machine type . # !pip install librosa # !pip install fastbook # !pip install gradio . . import pandas as pd from fastai.vision.all import * from fastai.data.all import * import matplotlib.pyplot as plt from matplotlib.pyplot import specgram import librosa import librosa.display import numpy as np from pathlib import Path import os import random import IPython from tqdm import tqdm from sklearn.metrics import accuracy_score import gradio as gr from collections import OrderedDict . . Custom Labelling Function For Classification . This function reads the categorisation information into a dictionary and then uses that filename lookup to recognise the class of a particular image . df = pd.read_csv(&#39;UrbanSound8K/metadata/UrbanSound8K.csv&#39;) #classification information across folds as provided from Urbansounds df[&#39;fname&#39;] = df[[&#39;slice_file_name&#39;,&#39;fold&#39;]].apply (lambda x: str(x[&#39;slice_file_name&#39;][:-4])+&#39;.png&#39;.strip(),axis=1 ) my_dict = dict(zip(df.fname,df[&#39;class&#39;])) def label_func(f_name): f_name = str(f_name).split(&#39;/&#39;)[-1:][0] return my_dict[f_name] . . File distribution across the folds . df.groupby([&#39;fold&#39;]).classID.count().sort_values(ascending=False).plot.bar() plt.ylabel(&#39;Files in each fold&#39;) plt.title(&#39;Files in each fold&#39;) . . Text(0.5, 1.0, &#39;Files in each fold&#39;) . Class distribution across the sound types . df.groupby(&#39;class&#39;).classID.count().sort_values(ascending=False).plot.bar() plt.ylabel(&#39;count&#39;) plt.title(&#39;Class distribution in the dataset&#39;) . . Text(0.5, 1.0, &#39;Class distribution in the dataset&#39;) . Model Build . Spider through all the folders for images (transformation of sound to melspectrograms is another notebook). . all_folds = list(np.arange(1,11)) all_folders = [str(i) for i in all_folds] image_files_loc = &#39;UrbanSoundTransforms/mel_spectrogram/&#39; all_files = get_image_files(image_files_loc,recurse=True, folders =all_folders ) . . Datablock with an 80-20 Random split on entire dataset . dblock = DataBlock(blocks=(ImageBlock,CategoryBlock), get_y = label_func, splitter = RandomSplitter(seed=1), ) dl = dblock.dataloaders(all_files) print (&#39;Train has {0} images and test has {1} images.&#39; .format(len(dl.train_ds),len(dl.valid_ds))) learn = vision_learner(dl, resnet34, metrics=accuracy) learn.fine_tune(3) . . Train has 6986 images and test has 1746 images. . epoch train_loss valid_loss accuracy time . 0 | 1.510322 | 0.698512 | 0.781787 | 00:31 | . epoch train_loss valid_loss accuracy time . 0 | 0.615340 | 0.356023 | 0.888889 | 00:39 | . 1 | 0.270150 | 0.213896 | 0.932990 | 00:40 | . 2 | 0.084770 | 0.181070 | 0.943299 | 00:40 | . Export the model . learn.export() . .",
            "url": "https://www.gputrain.com/sound/hugging%20face/fastai/2022/05/23/Urban-Sounds-8k-Classification-with-Hugging-Face-Demo.html",
            "relUrl": "/sound/hugging%20face/fastai/2022/05/23/Urban-Sounds-8k-Classification-with-Hugging-Face-Demo.html",
            "date": " • May 23, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Joe",
          "content": "Hello there. I am Joseph Matthew. . Welcome to my space, where I share some Jupyter notebook based experiments in my self-directed learning journey around machine learning and data wrangling. I enjoy developing solutions to real-world problems that leverage massive parallel architectures like the cloud, Apache Spark, and GPUs. . You can reach me on Twitter - @gputrain or LinkedIn. .",
          "url": "https://www.gputrain.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://www.gputrain.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}