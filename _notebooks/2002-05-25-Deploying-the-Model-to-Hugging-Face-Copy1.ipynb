{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c792ef",
   "metadata": {},
   "source": [
    "# 2 Deploying a Fast AI model to Hugging Face\n",
    "> Steps to get the hugging face gradio application to work\n",
    "\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [sound, hugging face, fastai]\n",
    "- image: images/melspectrogram.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff674515",
   "metadata": {},
   "source": [
    "## Know before getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d492e83",
   "metadata": {},
   "source": [
    "This notebook code is the same used to create my app.py file you can find in [UrbanSounds8k spaces repo](https://huggingface.co/spaces/gputrain/UrbanSounds8K/blob/main/app.py).\n",
    "\n",
    "Creating a space is simple and intuitive at Hugging Face. You will need a \"Create a new space\" and \"Create a new model repository\" at huggingfaces. You will find the interface to create a new model repository (repo) in your profile settings. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64ab8dd",
   "metadata": {},
   "source": [
    "![](images/createmodel.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ba750",
   "metadata": {},
   "source": [
    "If you upload your model artifacts into your spaces repository, you will run into 404 or 403 series errors. Once you create a model repo, the installation steps of Hugging Face will have an install [git lfs](https://git-lfs.github.com/) in the set of instructions specific to the location you clone this empty repo. \n",
    "\n",
    "\n",
    "If you upload your model artifacts into your spaces repository, you will run into 404 or 403 series errors. Once you create a model repo, the installation steps of Hugging Face will have an install git lfs in the set of instructions specific to the location you clone this empty repo.  \n",
    "\n",
    "Add in that repo before copying your model the *.pkl file from the earlier step; ensure you track pkl files as a type of file managed by Git LFS\n",
    "\n",
    "git lfs track \"*.pkl\"\n",
    "\n",
    "\n",
    "> Tip: Note what is my [spaces repo](https://huggingface.co/spaces/gputrain/UrbanSounds8K/tree/main) and what is in my [model repo](https://huggingface.co/gputrain/UrbanSound8K-model/tree/main)\n",
    "\n",
    "\n",
    "If you miss this step you will run into 403 errors as you execute this line:\n",
    "\n",
    "model_file = hf_hub_download(\"gputrain/UrbanSound8K-model\", \"model.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff69b5b",
   "metadata": {},
   "source": [
    "Back to spaces repo, for which I have specific requirements.txt to be able to load librosa modules to get my inference function to work. In my example, I also needed to get my labeller function to get my model to work. This [requirements.txt](https://huggingface.co/spaces/gputrain/UrbanSounds8K/blob/main/requirements.txt) is my spaces repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e4cdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71c862ec8aa423d8d046c295ff32a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f95c436434485b871e3c4b6139516d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/87.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "ref_file = hf_hub_download(\"gputrain/UrbanSound8K-model\", \"UrbanSound8K.csv\")\n",
    "\n",
    "model_file = hf_hub_download(\"gputrain/UrbanSound8K-model\", \"model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb96d626",
   "metadata": {},
   "source": [
    "## Labeller function\n",
    "\n",
    "My labeller function and loading of the model code are below. Model loaded and vocabulary on classes retrieved from the model object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0dae4939",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "df = pd.read_csv(ref_file) \n",
    "df['fname'] = df[['slice_file_name','fold']].apply (lambda x: str(x['slice_file_name'][:-4])+'.png'.strip(),axis=1 )\n",
    "my_dict = dict(zip(df.fname,df['class']))\n",
    "def label_func(f_name):\n",
    "    f_name = str(f_name).split('/')[-1:][0]\n",
    "    return my_dict[f_name]\n",
    "\n",
    "model = load_learner (model_file)\n",
    "labels = model.dls.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e381eb6",
   "metadata": {},
   "source": [
    "## Helpful external facing markdown text\n",
    "\n",
    "This bit of code allows some text markdown to appear in your demo. It takes standard markdown for tables and text formats, which is very useful to provide some descriptive elements to your demo, from my spaces repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7216fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "with open(\"article.md\") as f:\n",
    "    article = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13303315",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958c447c",
   "metadata": {},
   "source": [
    "#### Interface options \n",
    "\n",
    "The interface option allows for some text and examples objects which reside in your spaces repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8c761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "interface_options = {\n",
    "    \"title\": \"Urban Sound 8K Classification\",\n",
    "    \"description\": \"Fast AI example of using a pre-trained Resnet34 vision model for an audio classification task on the [Urban Sounds](https://urbansounddataset.weebly.com/urbansound8k.html) dataset. \",\n",
    "    \"article\": article,\n",
    "    \"interpretation\": \"default\",\n",
    "    \"layout\": \"horizontal\",\n",
    "    # Audio from validation file\n",
    "    \"examples\": [\"dog_bark.wav\", \"children_playing.wav\", \"air_conditioner.wav\", \"street_music.wav\", \"engine_idling.wav\",\n",
    "                \"jackhammer.wav\", \"drilling.wav\", \"siren.wav\",\"car_horn.wav\",\"gun_shot.wav\"],\n",
    "    \"allow_flagging\": \"never\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f275154",
   "metadata": {},
   "source": [
    "#### Pipeline helper function\n",
    "\n",
    "\n",
    "With my Urban Sound, 8K example, I have a custom transformation that takes the audio wav file and puts this through librosa to create a Melspectrogram. I am using a fixed temporary image as temp.png for my inference object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bbde66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "def convert_sounds_melspectogram (audio_file):\n",
    "\n",
    "    samples, sample_rate = librosa.load(audio_file)  #create onces with librosa\n",
    "\n",
    "    fig = plt.figure(figsize=[0.72,0.72])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    melS = librosa.feature.melspectrogram(y=samples, sr=sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(melS, ref=np.max))\n",
    "    filename  = 'temp.png'\n",
    "    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "    plt.close('all')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3804a6",
   "metadata": {},
   "source": [
    "#### Pipeline predict function\n",
    "\n",
    "The predict function uses this temp.png file for its predictions, and the labels_probs object has a list of class probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b3174c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "def predict():\n",
    "    img = PILImage.create('temp.png')\n",
    "    pred,pred_idx,probs = model.predict(img)\n",
    "    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n",
    "    return labels_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2742832",
   "metadata": {},
   "source": [
    "#### The Pipeleine function\n",
    "\n",
    "My inference pipeline is done in this end2endpipeline object that converts the sound and returns the output of my predict function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89bfd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "def end2endpipeline(filename):\n",
    "    convert_sounds_melspectogram(filename)\n",
    "    return predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e3a7b3",
   "metadata": {},
   "source": [
    "#### Launch options\n",
    "\n",
    "I call the gradio Interface object referencing my inference pipeline displaying input types and output classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b4230c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
      "  warnings.warn(value)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p38/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: `layout` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n"
     ]
    }
   ],
   "source": [
    "#collapse-output \n",
    "\n",
    "demo = gradio.Interface(\n",
    "    fn=end2endpipeline,\n",
    "    inputs=gradio.inputs.Audio(source=\"upload\", type=\"filepath\"),\n",
    "    outputs=gradio.outputs.Label(num_top_classes=10),\n",
    "    **interface_options,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141b47a1",
   "metadata": {},
   "source": [
    "#### Gradio local or public endpoint \n",
    "\n",
    "The launch options launches the gradio UI.  When using AWS I wasn't able to get this to work with the local URL, however if you use the share=True you get a gradio URL that works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "437b6570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861/\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"http://127.0.0.1:7861/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f67101bffa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f671108ce50>, 'http://127.0.0.1:7861/', None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "launch_options = {\n",
    "    \"enable_queue\": True,\n",
    "    \"share\": False,\n",
    "    #\"cache_examples\": True,\n",
    "}\n",
    "\n",
    "demo.launch(**launch_options)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01bcf71",
   "metadata": {},
   "source": [
    "## Hugging Face \n",
    "\n",
    "The final end point once you assemble all this work through the logs (to resolve errors) is [here](https://huggingface.co/spaces/gputrain/UrbanSounds8K)\n",
    "\n",
    "![](images/logs.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b3780",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p38",
   "language": "python",
   "name": "conda_pytorch_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
